{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nall written function are here \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "all written function are here \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded columns (including label): {'Sport', 'Dir', 'SrcAddr', 'DstMac', 'Label', 'Flgs', 'SrcMac', 'Attack Category', 'DstAddr'}\n",
      "Index(['pLoss', 'pDstLoss', 'Loss', 'SIntPktAct', 'pSrcLoss', 'SrcJitter',\n",
      "       'DstBytes', 'DstJitter', 'Rate', 'SrcBytes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# cleaning and pre processing data\n",
    "\n",
    "dataPath = \"data.csv\"\n",
    "data = pd.read_csv(dataPath)\n",
    "\n",
    "# seperating features\n",
    "label_column = 'Label' \n",
    "data_features = data.drop(columns=[label_column]).select_dtypes(include=['float64', 'int64'])\n",
    "Labels = data[label_column]\n",
    "# Add this to see excluded columns\n",
    "excluded_columns = set(data.columns) - set(data_features.columns)\n",
    "print(\"Excluded columns (including label):\", excluded_columns)\n",
    "\n",
    "#standardize\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_features)\n",
    "\n",
    "top_10_features_list_RF = ['DIntPkt', 'Load', 'DstJitter', 'Dur', 'SrcLoad', \n",
    "                     'Rate', 'Packet_num', 'SIntPkt', 'DstLoad', 'SrcJitter']\n",
    "top_10_features_rf = pd.DataFrame(data_features, columns=top_10_features_list_RF)\n",
    "top_5_features_rf = pd.DataFrame(data_features, columns=top_10_features_list_RF[:5])\n",
    "\n",
    "top_10_feaures_list_LDA =  ['pLoss','pDstLoss','Loss','SIntPktAct','pSrcLoss','SrcJitter', 'DstBytes','DstJitter',\n",
    "                            'Rate','SrcBytes']\n",
    "top_10_features_LDA = pd.DataFrame(data_features, columns=top_10_feaures_list_LDA)\n",
    "top_5_features_LDA = pd.DataFrame(data_features, columns=top_10_feaures_list_LDA[:5])\n",
    "                                   \n",
    "\n",
    "print(top_10_features_LDA.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_results(confusion_matrix, classification_report, accuracy):\n",
    "    #prints the cm results, classification report, and accuracy\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "    total = tn + fp + fn + tp\n",
    "    \n",
    "    # Calculate percentages\n",
    "    tn_pct = (tn/total) * 100\n",
    "    fp_pct = (fp/total) * 100\n",
    "    fn_pct = (fn/total) * 100\n",
    "    tp_pct = (tp/total) * 100\n",
    "    \n",
    "    # Print confusion matrix results\n",
    "    print(\"\\nConfusion Matrix Results:\")\n",
    "    print(f\"\"\"\n",
    "    True Negatives: {tn} ({tn_pct:.2f}%)\n",
    "    False Positives: {fp} ({fp_pct:.2f}%)\n",
    "    False Negatives: {fn} ({fn_pct:.2f}%)\n",
    "    True Positives: {tp} ({tp_pct:.2f}%)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report)\n",
    "    \n",
    "    # Print accuracy\n",
    "    print(f\"\\nModel Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_features(data_scaled, features, n):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    components = pca.components_\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    feature_importance = pd.DataFrame()\n",
    "    for i in range(n):\n",
    "        feature_importance[f'PC{i+1}'] = abs(components[i])\n",
    "    \n",
    "    feature_importance.index = features.columns\n",
    "    feature_importance['Overall_Importance'] = feature_importance.sum(axis=1)\n",
    "    feature_importance = feature_importance.sort_values('Overall_Importance', ascending=False)\n",
    "\n",
    "    print(f\"Total explained variance ratio: {sum(explained_variance):.2%}\")\n",
    "    print(\"\\nExplained variance ratio by component:\")\n",
    "    for i, var in enumerate(explained_variance):\n",
    "        print(f\"PC{i+1}: {var:.2%}\")\n",
    "    \n",
    "    print(f\"\\nTop {n} most important features:\")\n",
    "    top_n_features = feature_importance['Overall_Importance'].head(n)\n",
    "    print(top_n_features)\n",
    "    \n",
    "    # Add semicolon to suppress output\n",
    "    return feature_importance, top_n_features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total explained variance ratio: 87.71%\n",
      "\n",
      "Explained variance ratio by component:\n",
      "PC1: 24.29%\n",
      "PC2: 14.41%\n",
      "PC3: 12.17%\n",
      "PC4: 8.41%\n",
      "PC5: 8.28%\n",
      "PC6: 5.97%\n",
      "PC7: 3.95%\n",
      "PC8: 3.71%\n",
      "PC9: 3.33%\n",
      "PC10: 3.18%\n",
      "\n",
      "Top 10 most important features:\n",
      "Temp          1.598481\n",
      "SrcBytes      1.539131\n",
      "ST            1.538129\n",
      "Pulse_Rate    1.483176\n",
      "Heart_rate    1.481323\n",
      "SYS           1.459602\n",
      "sMinPktSz     1.453595\n",
      "SIntPkt       1.451182\n",
      "SrcLoad       1.441597\n",
      "DIA           1.384256\n",
      "Name: Overall_Importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "featureImportance, top_n = find_top_n_features(data_scaled, data_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(data_scaled, features, labels, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on PCA components using selected features\n",
    "    \n",
    "    Args:\n",
    "        data_scaled: scaled numpy array from StandardScaler\n",
    "        features: DataFrame with only the features to analyze\n",
    "        labels: target labels\n",
    "        n_components: number of PCA components to use\n",
    "    \"\"\"\n",
    "    print(\"\\nperformin k means on this many features: \", len(features.columns))\n",
    "    \n",
    "    \n",
    "    # Convert data_scaled to DataFrame with feature names\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    \n",
    "    # Select only the specified features\n",
    "    selected_features = scaled_df[features.columns]\n",
    "    \n",
    "    # PCA on selected features\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pc_features = pca.fit_transform(selected_features)\n",
    "    \n",
    "    pca_df = pd.DataFrame(\n",
    "        data=pc_features,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        pca_df, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    y_pred = kmeans.predict(X_test)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)    \n",
    "    accuracy = accuracy_score(y_test, y_pred)    \n",
    "    return cm, class_report, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "performin k means on this many features:  36\n",
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 4250 (86.81%)\n",
      "    False Positives: 2 (0.04%)\n",
      "    False Negatives: 644 (13.15%)\n",
      "    True Positives: 0 (0.00%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      4252\n",
      "           1       0.00      0.00      0.00       644\n",
      "\n",
      "    accuracy                           0.87      4896\n",
      "   macro avg       0.43      0.50      0.46      4896\n",
      "weighted avg       0.75      0.87      0.81      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 86.81%\n"
     ]
    }
   ],
   "source": [
    "cm, class_report, accuracy = perform_kmeans_clustering(data_scaled, data_features, data[label_column], n_components=3)\n",
    "\n",
    "print_classification_results(cm, class_report, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance, top_features = find_top_n_features(data_scaled, features, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(data_scaled, Labels, features, n_estimators):\n",
    "    \"\"\"\n",
    "    Train Random Forest on selected features\n",
    "    \n",
    "    Args:\n",
    "        data_scaled: scaled numpy array from StandardScaler\n",
    "        Labels: target labels \n",
    "        features: DataFrame with features to analyze\n",
    "        n_estimators: number of trees in forest\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report, accuracy)\n",
    "    \"\"\"\n",
    "    print(f\"Training Random Forest with {n_estimators} estimators\")\n",
    "    print(f\"Number of features used: {len(features.columns)}\")\n",
    "    \n",
    "    # Convert scaled data to DataFrame with feature names\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    \n",
    "    # Select only specified features\n",
    "    selected_features = scaled_df[features.columns]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        selected_features, Labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features.columns,\n",
    "        'importance': rf_classifier.feature_importances_\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return cm, class_report, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 36\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "       feature  importance\n",
      "8      DIntPkt    0.133990\n",
      "21        Load    0.096434\n",
      "12   DstJitter    0.093912\n",
      "17         Dur    0.086107\n",
      "3      SrcLoad    0.082480\n",
      "26        Rate    0.081926\n",
      "27  Packet_num    0.071981\n",
      "7      SIntPkt    0.067164\n",
      "4      DstLoad    0.056263\n",
      "11   SrcJitter    0.050777\n",
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 4241 (86.62%)\n",
      "    False Positives: 11 (0.22%)\n",
      "    False Negatives: 313 (6.39%)\n",
      "    True Positives: 331 (6.76%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4252\n",
      "           1       0.97      0.51      0.67       644\n",
      "\n",
      "    accuracy                           0.93      4896\n",
      "   macro avg       0.95      0.76      0.82      4896\n",
      "weighted avg       0.94      0.93      0.92      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 93.38%\n"
     ]
    }
   ],
   "source": [
    "forest_cm, forest_report, forest_accuracy = train_random_forest(data_scaled, Labels, data_features, 100)\n",
    "\n",
    "print_classification_results(forest_cm, forest_report, forest_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda(data_scaled, labels, features, top_n=10):\n",
    "    \"\"\"\n",
    "    Perform Linear Discriminant Analysis on selected features and optionally find top n important features\n",
    "    \n",
    "    Args:\n",
    "        data_scaled: scaled numpy array from StandardScaler\n",
    "        labels: target labels\n",
    "        features: DataFrame with features to analyze\n",
    "        top_n: number of top features to return based on LDA coefficients\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report, accuracy, top_features)\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming LDA on {len(features.columns)} features\")\n",
    "    \n",
    "    # Convert scaled data to DataFrame with feature names\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    \n",
    "    # Select only specified features\n",
    "    selected_features = scaled_df[features.columns].values\n",
    "    \n",
    "    # Split using the features DataFrame\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        selected_features, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and fit LDA model\n",
    "    lda = LDA()\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    # Print LDA components information\n",
    "    print(\"\\nLDA Components Information:\")\n",
    "    print(f\"Number of components: {lda.n_components}\")\n",
    "    print(\"\\nExplained variance ratio:\")\n",
    "    print(lda.explained_variance_ratio_)\n",
    "    \n",
    "    # Print component coefficients\n",
    "    print(\"\\nLinear Discriminant Coefficients:\")\n",
    "    for i, component in enumerate(lda.coef_):\n",
    "        print(f\"\\nLD{i+1} coefficients:\")\n",
    "        for feat, coef in zip(features.columns, component):\n",
    "            print(f\"{feat}: {coef:.4f}\")\n",
    "\n",
    "    # Make predictions and calculate metrics\n",
    "    y_pred = lda.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Find top n important features if requested\n",
    "    top_features = None\n",
    "    if top_n:\n",
    "        # Sum absolute coefficients across all discriminant functions\n",
    "        feature_importance = pd.Series(np.sum(np.abs(lda.coef_), axis=0), index=features.columns)\n",
    "        top_features = feature_importance.nlargest(top_n)\n",
    "        print(f\"\\nTop {top_n} most important features:\")\n",
    "        print(top_features)\n",
    "    \n",
    "    return conf_matrix, class_report, accuracy, top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 36 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "Dport: 0.0000\n",
      "SrcBytes: 4.5746\n",
      "DstBytes: -9.3746\n",
      "SrcLoad: -0.4146\n",
      "DstLoad: -3.5580\n",
      "SrcGap: -0.0000\n",
      "DstGap: 0.0000\n",
      "SIntPkt: -2.3615\n",
      "DIntPkt: -0.4477\n",
      "SIntPktAct: 453.4666\n",
      "DIntPktAct: -0.0000\n",
      "SrcJitter: -153.6353\n",
      "DstJitter: 7.8913\n",
      "sMaxPktSz: 0.0185\n",
      "dMaxPktSz: -0.9048\n",
      "sMinPktSz: -0.7674\n",
      "dMinPktSz: 0.0000\n",
      "Dur: 2.0393\n",
      "Trans: 0.0000\n",
      "TotPkts: 2.6564\n",
      "TotBytes: -1.0525\n",
      "Load: -2.3268\n",
      "Loss: -552.0637\n",
      "pLoss: 1173.2676\n",
      "pSrcLoss: -384.6796\n",
      "pDstLoss: -600.8920\n",
      "Rate: 4.7021\n",
      "Packet_num: -0.0375\n",
      "Temp: -0.0457\n",
      "SpO2: 0.0027\n",
      "Pulse_Rate: 0.2497\n",
      "SYS: -0.0287\n",
      "DIA: 0.1639\n",
      "Heart_rate: 0.0998\n",
      "Resp_Rate: 0.0286\n",
      "ST: 0.2382\n",
      "\n",
      "Top 10 most important features:\n",
      "pLoss         1173.267597\n",
      "pDstLoss       600.891950\n",
      "Loss           552.063711\n",
      "SIntPktAct     453.466610\n",
      "pSrcLoss       384.679565\n",
      "SrcJitter      153.635281\n",
      "DstBytes         9.374628\n",
      "DstJitter        7.891311\n",
      "Rate             4.702067\n",
      "SrcBytes         4.574640\n",
      "dtype: float64\n",
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 4233 (86.46%)\n",
      "    False Positives: 19 (0.39%)\n",
      "    False Negatives: 351 (7.17%)\n",
      "    True Positives: 293 (5.98%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      4252\n",
      "           1       0.94      0.45      0.61       644\n",
      "\n",
      "    accuracy                           0.92      4896\n",
      "   macro avg       0.93      0.73      0.79      4896\n",
      "weighted avg       0.93      0.92      0.91      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 92.44%\n"
     ]
    }
   ],
   "source": [
    "cm, report, accuracy, topFeatures = perform_lda(data_scaled, Labels, data_features, top_n=10)\n",
    "\n",
    "print_classification_results(cm, report, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(data_scaled, labels, features):\n",
    "    \"\"\"\n",
    "    Perform Gaussian Naive Bayes on selected features\n",
    "    \n",
    "    Args:\n",
    "        data_scaled: scaled numpy array from StandardScaler\n",
    "        labels: target labels\n",
    "        features: DataFrame with features to analyze\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report, accuracy)\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming Gaussian Naive Bayes on {len(features.columns)} features\")\n",
    "    \n",
    "    # Convert scaled data to DataFrame with feature names\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    \n",
    "    # Select only specified features\n",
    "    selected_features = scaled_df[features.columns]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        selected_features, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize and fit GNB model\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and calculate metrics\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return cm, class_report, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Gaussian Naive Bayes on 36 features\n",
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 3843 (78.49%)\n",
      "    False Positives: 409 (8.35%)\n",
      "    False Negatives: 338 (6.90%)\n",
      "    True Positives: 306 (6.25%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      4252\n",
      "           1       0.43      0.48      0.45       644\n",
      "\n",
      "    accuracy                           0.85      4896\n",
      "   macro avg       0.67      0.69      0.68      4896\n",
      "weighted avg       0.85      0.85      0.85      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 84.74%\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "cm, report, acc = gaussian_naive_bayes(data_scaled, Labels, data_features)\n",
    "print_classification_results(cm, report, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVC(data_scaled, features, labels):\n",
    "    \"\"\"\n",
    "    Train SVC models with different kernels using selected features\n",
    "    \n",
    "    Args:\n",
    "        data_scaled: scaled numpy array from StandardScaler\n",
    "        features: DataFrame with features to analyze\n",
    "        labels: target labels\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report, accuracy) for best kernel\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming SVC on this many features: \", len(features.columns))\n",
    "    \n",
    "    # Convert data_scaled to DataFrame with feature names\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    \n",
    "    # Select only specified features\n",
    "    selected_features = scaled_df[features.columns]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        selected_features, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    kernelTypes = ['linear', 'rbf', 'poly']\n",
    "    best_accuracy = 0\n",
    "    best_metrics = None\n",
    "    \n",
    "    print(\"\\nTraining SVC models with different kernels...\")\n",
    "    for kernelType in tqdm(kernelTypes, desc='Training'):\n",
    "        model = SVC(kernel=kernelType, C=1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\nKernel: {kernelType}\")\n",
    "        print(f\"Accuracy: {accuracy:.5f}\")\n",
    "        \n",
    "    return cm, report, accuracy\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing SVC on this many features:  36\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:06<00:13,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.92586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:08<00:03,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.92933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:10<00:00,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.92851\n",
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 4244 (86.68%)\n",
      "    False Positives: 8 (0.16%)\n",
      "    False Negatives: 342 (6.99%)\n",
      "    True Positives: 302 (6.17%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4252\n",
      "           1       0.97      0.47      0.63       644\n",
      "\n",
      "    accuracy                           0.93      4896\n",
      "   macro avg       0.95      0.73      0.80      4896\n",
      "weighted avg       0.93      0.93      0.92      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 92.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "cm, report, acc = train_SVC(data_scaled, data_features, Labels)\n",
    "print_classification_results(cm, report, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(data_scaled, labels, features):\n",
    "    \"\"\"\n",
    "    Perform CNN classification on selected features with adaptive architecture\n",
    "    \"\"\"\n",
    "    print(f\"\\nPerforming CNN on {len(features.columns)} features\")\n",
    "    \n",
    "    # Data preparation (keep existing code until reshape)\n",
    "    scaled_df = pd.DataFrame(data_scaled, columns=features.columns)\n",
    "    selected_features = scaled_df[features.columns].values\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(labels)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "    # Reshape data\n",
    "    num_features = selected_features.shape[1]\n",
    "    grid_size = int(np.ceil(np.sqrt(num_features)))\n",
    "    X_padded = np.zeros((selected_features.shape[0], grid_size * grid_size))\n",
    "    X_padded[:, :num_features] = selected_features\n",
    "    X_reshaped = X_padded.reshape(-1, grid_size, grid_size, 1)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_reshaped, y_categorical, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Build adaptive CNN model based on input size\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv2D layer with smaller kernel and no pooling for small inputs\n",
    "    if grid_size < 5:\n",
    "        model.add(Conv2D(32, kernel_size=(2,2), activation='relu', \n",
    "                        input_shape=(grid_size, grid_size, 1),\n",
    "                        padding='same'))\n",
    "    else:\n",
    "        model.add(Conv2D(32, kernel_size=(3,3), activation='relu', \n",
    "                        input_shape=(grid_size, grid_size, 1),\n",
    "                        padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        \n",
    "        # Add second conv layer only for larger inputs\n",
    "        if grid_size >= 7:\n",
    "            model.add(Conv2D(64, kernel_size=(2,2), activation='relu', padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))  # Reduced dense layer size\n",
    "    model.add(Dropout(0.3))  # Reduced dropout\n",
    "    model.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, \n",
    "                       validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    # Predictions and metrics (keep existing code)\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_test_labels, y_pred)\n",
    "    class_report = classification_report(y_test_labels, y_pred)\n",
    "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "    \n",
    "    return cm, class_report, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing CNN on 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix Results:\n",
      "\n",
      "    True Negatives: 4237 (86.54%)\n",
      "    False Positives: 15 (0.31%)\n",
      "    False Negatives: 312 (6.37%)\n",
      "    True Positives: 332 (6.78%)\n",
      "    \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4252\n",
      "           1       0.96      0.52      0.67       644\n",
      "\n",
      "    accuracy                           0.93      4896\n",
      "   macro avg       0.94      0.76      0.82      4896\n",
      "weighted avg       0.93      0.93      0.92      4896\n",
      "\n",
      "\n",
      "Model Accuracy: 93.32%\n"
     ]
    }
   ],
   "source": [
    "# Run the CNN model\n",
    "cnn_cm, cnn_report, cnn_accuracy = CNN(data_scaled, Labels, data_features)\n",
    "\n",
    "# Print the results\n",
    "print_classification_results(cnn_cm, cnn_report, cnn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 36 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "Dport: 0.0000\n",
      "SrcBytes: 4.5746\n",
      "DstBytes: -9.3746\n",
      "SrcLoad: -0.4146\n",
      "DstLoad: -3.5580\n",
      "SrcGap: -0.0000\n",
      "DstGap: 0.0000\n",
      "SIntPkt: -2.3615\n",
      "DIntPkt: -0.4477\n",
      "SIntPktAct: 453.4666\n",
      "DIntPktAct: -0.0000\n",
      "SrcJitter: -153.6353\n",
      "DstJitter: 7.8913\n",
      "sMaxPktSz: 0.0185\n",
      "dMaxPktSz: -0.9048\n",
      "sMinPktSz: -0.7674\n",
      "dMinPktSz: 0.0000\n",
      "Dur: 2.0393\n",
      "Trans: 0.0000\n",
      "TotPkts: 2.6564\n",
      "TotBytes: -1.0525\n",
      "Load: -2.3268\n",
      "Loss: -552.0637\n",
      "pLoss: 1173.2676\n",
      "pSrcLoss: -384.6796\n",
      "pDstLoss: -600.8920\n",
      "Rate: 4.7021\n",
      "Packet_num: -0.0375\n",
      "Temp: -0.0457\n",
      "SpO2: 0.0027\n",
      "Pulse_Rate: 0.2497\n",
      "SYS: -0.0287\n",
      "DIA: 0.1639\n",
      "Heart_rate: 0.0998\n",
      "Resp_Rate: 0.0286\n",
      "ST: 0.2382\n",
      "\n",
      "Top 10 most important features:\n",
      "pLoss         1173.267597\n",
      "pDstLoss       600.891950\n",
      "Loss           552.063711\n",
      "SIntPktAct     453.466610\n",
      "pSrcLoss       384.679565\n",
      "SrcJitter      153.635281\n",
      "DstBytes         9.374628\n",
      "DstJitter        7.891311\n",
      "Rate             4.702067\n",
      "SrcBytes         4.574640\n",
      "dtype: float64\n",
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 36\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "       feature  importance\n",
      "8      DIntPkt    0.133990\n",
      "21        Load    0.096434\n",
      "12   DstJitter    0.093912\n",
      "17         Dur    0.086107\n",
      "3      SrcLoad    0.082480\n",
      "26        Rate    0.081926\n",
      "27  Packet_num    0.071981\n",
      "7      SIntPkt    0.067164\n",
      "4      DstLoad    0.056263\n",
      "11   SrcJitter    0.050777\n",
      "\n",
      "performin k means on this many features:  36\n",
      "\n",
      "Performing SVC on this many features:  36\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:06<00:13,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.92586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:08<00:03,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.92933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:10<00:00,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.92851\n",
      "\n",
      "Performing Gaussian Naive Bayes on 36 features\n",
      "\n",
      "Performing CNN on 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 5 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "pLoss: 3.4085\n",
      "pDstLoss: -2.2251\n",
      "Loss: -0.3130\n",
      "SIntPktAct: 0.4211\n",
      "pSrcLoss: -1.6207\n",
      "\n",
      "Top 10 most important features:\n",
      "pLoss         3.408538\n",
      "pDstLoss      2.225052\n",
      "pSrcLoss      1.620706\n",
      "SIntPktAct    0.421074\n",
      "Loss          0.313039\n",
      "dtype: float64\n",
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 5\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "      feature  importance\n",
      "1    pDstLoss    0.326323\n",
      "0       pLoss    0.310721\n",
      "2        Loss    0.258536\n",
      "4    pSrcLoss    0.088448\n",
      "3  SIntPktAct    0.015973\n",
      "\n",
      "performin k means on this many features:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing SVC on this many features:  5\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Training:  33%|███▎      | 1/3 [00:00<00:01,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.86846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Training:  67%|██████▋   | 2/3 [00:02<00:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.86846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Training: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.86846\n",
      "\n",
      "Performing Gaussian Naive Bayes on 5 features\n",
      "\n",
      "Performing CNN on 5 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 10 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "pLoss: 393.8038\n",
      "pDstLoss: -201.8607\n",
      "Loss: -185.6725\n",
      "SIntPktAct: 153.4113\n",
      "pSrcLoss: -129.0672\n",
      "SrcJitter: -52.4839\n",
      "DstBytes: -1.7181\n",
      "DstJitter: 2.5665\n",
      "Rate: -1.5558\n",
      "SrcBytes: 1.2818\n",
      "\n",
      "Top 10 most important features:\n",
      "pLoss         393.803799\n",
      "pDstLoss      201.860726\n",
      "Loss          185.672451\n",
      "SIntPktAct    153.411333\n",
      "pSrcLoss      129.067201\n",
      "SrcJitter      52.483944\n",
      "DstJitter       2.566473\n",
      "DstBytes        1.718067\n",
      "Rate            1.555772\n",
      "SrcBytes        1.281833\n",
      "dtype: float64\n",
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 10\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "      feature  importance\n",
      "7   DstJitter    0.440178\n",
      "8        Rate    0.337131\n",
      "5   SrcJitter    0.208638\n",
      "6    DstBytes    0.006746\n",
      "9    SrcBytes    0.006033\n",
      "0       pLoss    0.000368\n",
      "4    pSrcLoss    0.000365\n",
      "2        Loss    0.000298\n",
      "1    pDstLoss    0.000195\n",
      "3  SIntPktAct    0.000049\n",
      "\n",
      "performin k means on this many features:  10\n",
      "\n",
      "Performing SVC on this many features:  10\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.92688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:02<00:01,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.92892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.92729\n",
      "\n",
      "Performing Gaussian Naive Bayes on 10 features\n",
      "\n",
      "Performing CNN on 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 5 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "DIntPkt: -0.6907\n",
      "Load: -1.0650\n",
      "DstJitter: 1.8382\n",
      "Dur: -0.6179\n",
      "SrcLoad: -0.2895\n",
      "\n",
      "Top 10 most important features:\n",
      "DstJitter    1.838189\n",
      "Load         1.064962\n",
      "DIntPkt      0.690748\n",
      "Dur          0.617912\n",
      "SrcLoad      0.289481\n",
      "dtype: float64\n",
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 5\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "     feature  importance\n",
      "0    DIntPkt    0.250659\n",
      "2  DstJitter    0.249182\n",
      "4    SrcLoad    0.214627\n",
      "3        Dur    0.151189\n",
      "1       Load    0.134344\n",
      "\n",
      "performin k means on this many features:  5\n",
      "\n",
      "Performing SVC on this many features:  5\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:02<00:04,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.92586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:04<00:01,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.92729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.92872\n",
      "\n",
      "Performing Gaussian Naive Bayes on 5 features\n",
      "\n",
      "Performing CNN on 5 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing LDA on 10 features\n",
      "\n",
      "LDA Components Information:\n",
      "Number of components: None\n",
      "\n",
      "Explained variance ratio:\n",
      "[1.]\n",
      "\n",
      "Linear Discriminant Coefficients:\n",
      "\n",
      "LD1 coefficients:\n",
      "DIntPkt: 0.0939\n",
      "Load: -22.3502\n",
      "DstJitter: 2.3100\n",
      "Dur: -1.2466\n",
      "SrcLoad: 0.0074\n",
      "Rate: 57.9967\n",
      "Packet_num: -0.1367\n",
      "SIntPkt: -0.9139\n",
      "DstLoad: -39.6188\n",
      "SrcJitter: -0.2130\n",
      "\n",
      "Top 10 most important features:\n",
      "Rate          57.996661\n",
      "DstLoad       39.618788\n",
      "Load          22.350241\n",
      "DstJitter      2.309951\n",
      "Dur            1.246617\n",
      "SIntPkt        0.913868\n",
      "SrcJitter      0.213015\n",
      "Packet_num     0.136689\n",
      "DIntPkt        0.093929\n",
      "SrcLoad        0.007363\n",
      "dtype: float64\n",
      "Training Random Forest with 100 estimators\n",
      "Number of features used: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "      feature  importance\n",
      "0     DIntPkt    0.179262\n",
      "2   DstJitter    0.167164\n",
      "4     SrcLoad    0.122655\n",
      "6  Packet_num    0.111005\n",
      "9   SrcJitter    0.078561\n",
      "3         Dur    0.075984\n",
      "7     SIntPkt    0.068331\n",
      "5        Rate    0.068188\n",
      "1        Load    0.064495\n",
      "8     DstLoad    0.064356\n",
      "\n",
      "performin k means on this many features:  10\n",
      "\n",
      "Performing SVC on this many features:  10\n",
      "\n",
      "Training SVC models with different kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 1/3 [00:01<00:03,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: linear\n",
      "Accuracy: 0.92525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 2/3 [00:03<00:01,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: rbf\n",
      "Accuracy: 0.92708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kernel: poly\n",
      "Accuracy: 0.92708\n",
      "\n",
      "Performing Gaussian Naive Bayes on 10 features\n",
      "\n",
      "Performing CNN on 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\allen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   LDA Random Forest K-Means Clustering       SVM  \\\n",
      "All Features  0.924428      0.933824           0.868056  0.928513   \n",
      "Top 5 LDA     0.868464      0.868464           0.868056  0.868464   \n",
      "Top 10 LDA    0.925858      0.919322           0.868056  0.927288   \n",
      "Top 5 RF      0.910948      0.921773           0.831291  0.928717   \n",
      "Top 10 RF     0.915237        0.9279           0.868464  0.927083   \n",
      "\n",
      "             Gaussian Naive Bayes       CNN  \n",
      "All Features             0.847426  0.931781  \n",
      "Top 5 LDA                0.132966  0.868464  \n",
      "Top 10 LDA               0.132966  0.928922  \n",
      "Top 5 RF                 0.917688  0.926879  \n",
      "Top 10 RF                0.866013  0.929126  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_sets = {\n",
    "    'All Features': data_features,\n",
    "    'Top 5 LDA': top_5_features_LDA,\n",
    "    'Top 10 LDA': top_10_features_LDA,\n",
    "    'Top 5 RF': top_5_features_rf,\n",
    "    'Top 10 RF': top_10_features_rf\n",
    "}\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results_df = pd.DataFrame(index=feature_sets.keys(), columns=['LDA', 'Random Forest', 'K-Means Clustering', 'SVM', 'Gaussian Naive Bayes', 'CNN'])\n",
    "\n",
    "# Run each model with each feature set\n",
    "for feature_set_name, feature_set in feature_sets.items():\n",
    "    # Select the features\n",
    "    selected_features = data_features[feature_set.columns]\n",
    "    selected_data_scaled = scaler.fit_transform(selected_features)\n",
    "    \n",
    "    # Split the data\n",
    "    \n",
    "    # LDA\n",
    "    cm, report, accuracy, _ = perform_lda(selected_data_scaled, Labels, selected_features, top_n=10)\n",
    "    results_df.loc[feature_set_name, 'LDA'] = accuracy\n",
    "    \n",
    "    # Random Forest\n",
    "    cm, report, accuracy = train_random_forest(selected_data_scaled, Labels, selected_features, 100)\n",
    "    results_df.loc[feature_set_name, 'Random Forest'] = accuracy\n",
    "    \n",
    "    # K-Means Clustering\n",
    "    cm, report, accuracy = perform_kmeans_clustering(selected_data_scaled, selected_features, Labels, n_components=3)\n",
    "    results_df.loc[feature_set_name, 'K-Means Clustering'] = accuracy\n",
    "    \n",
    "    # SVM\n",
    "    cm, report, accuracy = train_SVC(selected_data_scaled, selected_features, Labels)\n",
    "    results_df.loc[feature_set_name, 'SVM'] = accuracy\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    cm, report, accuracy = gaussian_naive_bayes(selected_data_scaled, Labels, selected_features)\n",
    "    results_df.loc[feature_set_name, 'Gaussian Naive Bayes'] = accuracy\n",
    "    \n",
    "    # CNN\n",
    "    cm, report, accuracy = CNN(selected_data_scaled, Labels, selected_features)\n",
    "    results_df.loc[feature_set_name, 'CNN'] = accuracy\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
